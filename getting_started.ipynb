{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Perceptual Adversarial Robustness\n",
    "\n",
    "This notebook contains examples of how to load a pretrained model, measure LPIPS distance, and construct perceptual and non-perceptual attacks.\n",
    "\n",
    "If you are running this notebook in Google Colab, it is recommended to use a GPU. You can enable GPU acceleration by going to **Runtime** > **Change runtime type** and selecting **GPU** from the dropdown."
   ]
  },
  {
   "source": [
    "First, make sure you have installed the `perceptual_advex` package, either from GitHub or PyPI:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import perceptual_advex\n",
    "except ImportError:\n",
    "    !pip install perceptual-advex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pretrained model\n",
    "First, let's load the CIFAR-10 dataset along with a pretrained model. The following code will download a model checkpoint and load it, but you can change the `checkpoint_name` parameter to load a different checkpoint. The checkpoint we're downloading here is trained against $L_2$ adversarial attacks with bound $\\epsilon = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "if not os.path.exists('data/checkpoints/cifar_pgd_l2_1.pt'):\n",
    "    !mkdir -p data/checkpoints\n",
    "    !curl -o data/checkpoints/cifar_pgd_l2_1.pt https://perceptual-advex.s3.us-east-2.amazonaws.com/cifar_pgd_l2_1_cpu.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptual_advex.utilities import get_dataset_model\n",
    "\n",
    "dataset, model = get_dataset_model(\n",
    "    dataset='cifar',\n",
    "    arch='resnet50',\n",
    "    checkpoint_fname='data/checkpoints/cifar_pgd_l2_1.pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to experiment with ImageNet-100 instead, just change the above to\n",
    "\n",
    "    dataset, model = get_dataset_model(\n",
    "        dataset='imagenet100',\n",
    "        # Change this to where ImageNet is downloaded.\n",
    "        dataset_path='/path/to/imagenet',\n",
    "        arch='resnet50',\n",
    "        # Change this to a pretrained checkpoint path.\n",
    "        checkpoint_fname='/path/to/checkpoint',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing images in the dataset\n",
    "\n",
    "Now that we have a dataset and model loaded, we can view some images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use this helper function to show images in the Jupyter notebook.\n",
    "%matplotlib inline\n",
    "def show(img):\n",
    "    if len(img.size()) == 4:\n",
    "        img = torchvision.utils.make_grid(img, nrow=10, padding=0)\n",
    "    npimg = img.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(18,16), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a validation set loader.\n",
    "batch_size = 10\n",
    "_, val_loader = dataset.make_loaders(1, batch_size, only_val=True)\n",
    "\n",
    "# Get a batch from the validation set.\n",
    "inputs, labels = next(iter(val_loader))\n",
    "\n",
    "# If we have a GPU, let's convert everything to CUDA so it's quicker.\n",
    "if torch.cuda.is_available():\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    model.cuda()\n",
    "\n",
    "# Show the batch!\n",
    "show(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test the accuracy of the model on this set of inputs by comparing the model output to the ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model(inputs).argmax(1)\n",
    "print('Natural accuracy is', (labels == pred_labels).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the natural accuracy is very low on this batch of images, you might want to load a new set by re-running the two cells above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating perceptual adversarial examples\n",
    "\n",
    "Next, let's generate some perceptual adversarial examples using Lagrange perceptual attack (LPA) with AlexNet bound $\\epsilon = 0.5$. Other perceptual attacks (PPGD and Fast-LPA) are also found in the `perceptual_advex.perceptual_attacks` module, and they mostly share the same options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptual_advex.perceptual_attacks import LagrangePerceptualAttack\n",
    "\n",
    "attack = LagrangePerceptualAttack(\n",
    "    model,\n",
    "    num_iterations=10,\n",
    "    # The LPIPS distance bound on the adversarial examples.\n",
    "    bound=0.5,\n",
    "    # The model to use for calculate LPIPS; here we use AlexNet.\n",
    "    # You can also use 'self' to perform a self-bounded attack.\n",
    "    lpips_model='alexnet_cifar',\n",
    ")\n",
    "adv_inputs = attack(inputs, labels)\n",
    "\n",
    "# Show the adversarial examples.\n",
    "show(adv_inputs)\n",
    "\n",
    "# Show the magnified difference between the adversarial examples and unperturbed inputs.\n",
    "show((adv_inputs - inputs) * 5 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the perturbations are sometimes large, the adversarial examples are still recognizable as the original image and do not appear too different perceptually.\n",
    "\n",
    "We can calculate the accuracy of the classifier on the adversarial examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_pred_labels = model(adv_inputs).argmax(1)\n",
    "print('Adversarial accuracy is', (labels == adv_pred_labels).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this network has been trained to be robust to $L_2$ perturbations, there are still imperceptible perturbations found using LPA that fool it almost every time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating LPIPS distance\n",
    "\n",
    "Next, let's calculate the LPIPS distance between the adversarial examples we generated and the original inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptual_advex.distances import LPIPSDistance\n",
    "from perceptual_advex.perceptual_attacks import get_lpips_model\n",
    "\n",
    "# LPIPS is based on the activations of a classifier, so we need to first\n",
    "# load the classifier we'll use.\n",
    "lpips_model = get_lpips_model('alexnet_cifar')\n",
    "if torch.cuda.is_available():\n",
    "    lpips_model.cuda()\n",
    "\n",
    "# Now we can define a distance based on the model we loaded.\n",
    "# We could also do LPIPSDistance(model) for self-bounded LPIPS.\n",
    "lpips_distance = LPIPSDistance(lpips_model)\n",
    "\n",
    "# Finally, let's calculate the distance between the inputs and adversarial examples.\n",
    "print(lpips_distance(inputs, adv_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the distances are within the bound of 0.5! At this bound, the adversarial perturbations should all have a similar level of perceptibility to the human eye.\n",
    "\n",
    "Other distance measures between images are also defined in the `perceptual_advex.distances` package, including $L_\\infty$, $L_2$, and SSIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating non-perceptual adversarial examples\n",
    "\n",
    "The `perceptual_advex` package also includes code to perform attacks based on other, narrower threat models like $L_\\infty$ or $L_2$ distance and spatial transformations. The non-perceptual attacks are all in the `perceptual_advex.attacks` module. First, let's try an $L_2$ attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptual_advex.attacks import L2Attack\n",
    "\n",
    "attack = L2Attack(\n",
    "    model,\n",
    "    'cifar',\n",
    "    # The bound is divided by 255, so this is equivalent to eps=1.\n",
    "    bound=255,\n",
    ")\n",
    "l2_adv_inputs = attack(inputs, labels)\n",
    "\n",
    "show(l2_adv_inputs)\n",
    "show((l2_adv_inputs - inputs) * 5 + 0.5)\n",
    "\n",
    "l2_adv_pred_labels = model(l2_adv_inputs).argmax(1)\n",
    "print('L2 adversarial accuracy is', (labels == l2_adv_pred_labels).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a spatial attack (StAdv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptual_advex.attacks import StAdvAttack\n",
    "\n",
    "attack = StAdvAttack(\n",
    "    model,\n",
    "    bound=0.02,\n",
    ")\n",
    "spatial_adv_inputs = attack(inputs, labels)\n",
    "\n",
    "show(spatial_adv_inputs)\n",
    "show((spatial_adv_inputs - inputs) * 5 + 0.5)\n",
    "\n",
    "spatial_adv_pred_labels = model(spatial_adv_inputs).argmax(1)\n",
    "print('Spatial adversarial accuracy is', (labels == spatial_adv_pred_labels).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's pretty much it for how to use the package! As a final note, here is an overview of what each module contains:\n",
    "\n",
    " * `perceptual_advex.attacks`: non-perceptual attacks (e.g. $L_2$, $L_\\infty$, spatial, recoloring, JPEG, etc.)\n",
    " * `perceptual_advex.datasets`: datasets (e.g. ImageNet-100, CIFAR-10, etc.)\n",
    " * `perceptual_advex.distances`: distance measures between images (e.g. LPIPS, SSIM, $L_2$)\n",
    " * `perceptual_advex.evaluation`: functions used for evaluating a trained model against attacks\n",
    " * `perceptual_advex.models`: classifier architectures (e.g. ResNet, AlexNet, etc.)\n",
    " * `perceptual_advex.perceptual_attacks`: perceptual attacks (e.g. LPA, PPGD, Fast-LPA)\n",
    " * `perceptual_advex.trades_wrn`: classifier architecture used by the TRADES defense (Zhang et al.)\n",
    " * `perceptual_advex.utilites`: various utilites, including `get_dataset_model` function to load a dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}